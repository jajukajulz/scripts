{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Web Scraping in Python\n",
    "\n",
    "\n",
    "(*juliankanjere@gmail.com, May 2019*)\n",
    "\n",
    "This tutorial is designed with the aim of being a starting point into Web Scraping. You may be wondering, what is Web Scraping? The English definition of scraping (*according to Dr Google*) is to drag or pull a sharp implement across a surface so as to remove dirt or other matter. Web Scraping is therefore an automated way to render a Web Page and extract information from the Web Page using code. One might be wondering, why would there be a need to perform Web Scraping? The answer is simple, if one wanted to store data from a website locally in order to do some data analysis or aggregation of sort, it would be incredibly time consuming if they had to do so for a website with 1000 pages for example. One would need to load each page, perform a series of copy and paste and hope that they do not make a mistake. The alternative to manually doing this would be to make use of a Web Scraping application to automatically collect the data. This would not only save time but is likely to be more accurate than manual human effort.\n",
    "\n",
    "In this tutorial, we walk through an example of a simple Web Scraping application written in Python 3. We will be scraping a Web Page that has a list of different courses, their descriptions and additional information. Once we have scraped the course information, we persist the data in a csv file. \n",
    "\n",
    "This tutorial assumes some working knowledge of Python and Web Programming concepts (such as HTML, CSS, requests and responses).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This tutorial is broken up into sections that resemble the typical flow of a Web Scraping program written in Python, which is as follows:\n",
    "- Define a list URL's that point to the Web Pages that will be scraped.\n",
    "- Loop over the list of URL's and for each URL\n",
    "    - Issue a web request to the Web Server.\n",
    "    - Receive a response from the Web Server.\n",
    "    - Parse the response and store the parsed data into a data frame. \n",
    "- Persist the dataframe to the harddisk in the form (in the form of a CSV file or an Excel spreadsheet).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "The key python libraries used include:\n",
    "- `Pandas` - Pandas stands for Python Data Analysis Library. It is a library with powerful data structures for data analysis, time series and statistics.\n",
    "- `Beautiful Soup` - Screen scrapping Python library used for pulling data out of HTML and XML files.\n",
    "- `requests` - Python library for sending HTTP requests to a Web Server.\n",
    "- `time` - The Python time module provides many ways of representing time in code, such as objects, numbers, and strings.\n",
    "- `random` - Python module for generating pseudo-random numbers.\n",
    "- `os` - Python provides a way of using operating system dependent functionality such as interacting with the File System.\n",
    "\n",
    "These libraries are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Setup\n",
    "\n",
    "In this example, we will be scraping data from the African Institute of Financial Markets and Risk Management's (AIFMRM) list of online short courses. You can navigate to this [link](http://www.aifmrm.uct.ac.za/education/online-short-courses/ \"AIFMRM list of online short courses\") to see the courses and the details available for each course. You will see from this page that each cousrse has details such as Course Name, Course Description and Course Further Information. The Course Further Information consists of one or more links. \n",
    "\n",
    "![title](img/webscrap1.png)\n",
    "\n",
    "We would like to store Course Name, Course Description and Course Further Information in tabular form with one row per course.\n",
    "\n",
    "The initial setup involves the following:\n",
    "- Defining the URL for the website that we will be scraping from. See `BASE_URL` and `site_urls_list`.\n",
    "- Updating the User Agent HTTP header of the Web Request that we will be sending to the server when we access the URL specified above. This is done so that the Web Request appears to be coming from a Web Browser instead of a program. Some Web Servers are configured to deny Web Requests that do not come from Web Browser e.g. hackers write bots that try to brute force login on web applications. Therefore, if we do not set the User Agent to appear as if it is coming from a Web Browser, there is a chance that our Web Scraper may not be able to access online short course web page. See `headers`.\n",
    "- Defining the data buckets that will hold each of the course details of interest. Put another way, we setup a Python list to represent each column containing some detail of the course. Recall that the final output of this exercise should be the course information in tabular form with one course per row. The columns we have decided on include `title`, `description`, `source_url`, `further_info1_description`, `further_info1_url`, `further_info2_description`, `further_info2_url`, `further_info3_description`, `further_info3_url`, `further_info4_description`, `further_info4_url`. If you looked at this [link](http://www.aifmrm.uct.ac.za/education/online-short-courses/ \"AIFMRM list of online short courses\"), you will have noticed that each course generally has one or two items under Further Information. In our design, we make provision for upto four items, each item having a description (the text) and a URL (the link that the user is navigated to when they click on the item).\n",
    "- Setting up a counter for the number of Web Pages that have been scraped in order to give a summary at the end. See `n_pages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url we will be scrapping from\n",
    "BASE_URL = 'http://www.aifmrm.uct.ac.za/education/online-short-courses/'\n",
    "\n",
    "#Spoof the User Agent HTTP header so that queries look like they are coming from an actual browser instead of python-requests/version\n",
    "headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "\n",
    "# setting up the lists that will form pandas dataframe with all the results\n",
    "title = []\n",
    "description = []\n",
    "source_url = []\n",
    "further_info1_description = []\n",
    "further_info1_url = []\n",
    "further_info2_description = []\n",
    "further_info2_url = []\n",
    "further_info3_description = []\n",
    "further_info3_url = []\n",
    "further_info4_description = []\n",
    "further_info4_url = []\n",
    "\n",
    "n_pages = 0\n",
    "\n",
    "#list to hold the urls we will be scraping\n",
    "site_urls_list = []\n",
    "site_urls_list.append(BASE_URL)\n",
    "empty_list = [\"\",\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Web Request and Response\n",
    "\n",
    "At a very high level, when a user enters a URL into their browser , the browser (as an HTTP client) sends a request message to the HTTP server (where the web page/application resides). The server, in turn, returns a response message which is the web page that was requested. When Web Scraping, we make use of the Python `requests` library which sends the HTTP request and receives the HTTP response. Assumming that the request was successful and the server sends a response, the response is generally in the form of HTML (which in a Browser would be rendered in a well formatted way).\n",
    "\n",
    "In the code snippet that follows Section 5, you will notice that we perform some basic error handling using Pythons `try except`. We have set a timeout of 60 seconds, which it to say that if we do not recieve a response from the Web Server within 60 seconds, we should abort the operation. We also make provision for the case where there is an issue with the network and we do not recieve the response that we were expecting.  \n",
    "\n",
    "Once we have recieved the raw HTML response from the server, we convert it into a Beautiful Soup object. \n",
    "\n",
    "\n",
    "## 5. Parse Data\n",
    "\n",
    "Beautiful Soup is the backbone of the Web Scraping exercise. The `page_html` object that we have created is a Beautiful Soup object that can be queried based on the HTML elements it holds. Below is a snippet of the raw HTML that we will be parsing to return the course details\n",
    "![title](img/webscrap.png)\n",
    "\n",
    "Looking at the source HTML, one can see that:\n",
    "- the course information is found in a `div` whose CSS class is `course accordian`.\n",
    "- each course is self contained in `article` elements\n",
    "- Course Title is in a `label` element whose for attribute corresponds to the identifier of the course accordian e.g. *accordian_1*.\n",
    "- Course Description is in a `div` whose class is content.\n",
    "- Further information is in an unordered HTML list where each list entry is denoted by `li`.\n",
    "\n",
    "Using this information, we proceed to query the `page_html` Beautiful Soup object, starting by returning only the HTML within the `div` with class set to `course accordian`. This gives us the `course_section` which we then query for all `article` elements (there is one `article` element for each course). We finally iterate over all the courses to return the invdividual details of interest. For each of the course details of interest, we employ error handling because it is not always the case that the HTML is well formed (e.g. the Web Developer might not have been very thorough in labelling and identification of elements) and we would not want our Web Scraper to fall over when it comes across an irregularity. \n",
    "\n",
    "For each course, we return the details of interest and then we assign these details to the column buckets earlier mentioned i.e. we append the details to the respective Python list for the column. It follows that at the end of the exercise, each of the column lists are expected to have the same length and therefore it is necessary to update the lists with empty strings in cases that a column does not have the expectd data (this is particularly relevant for the further_information fields).\n",
    "\n",
    "In this example, since there are only three courses, all on one page, we are therefore only scraping one page. If there were dozens of courses spanning multiple pages (with a similar HTML format), our scraper would proceed onto the next page (assumming that the page would have been added to `site_urls_list` list). However, in an attempt not to exhaust a Web Servers resources with multiple requests in quick succession, it is good practise to add a delay to each request. In this example, we set a delay that is anything between one and two seconds between each successive Web Request using `time.sleep(randint(1, 2))`.\n",
    "\n",
    "Lastly, once we are done with the scraping, we print out a summary of the number of pages we have scraped and the number of courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Page: ' 1 \n",
      "'HTTP Status Code: ' 200 \n",
      "Summary: scraped 1 pages containing 3 courses.\n"
     ]
    }
   ],
   "source": [
    "for site_url in site_urls_list:\n",
    "    n_pages += 1\n",
    "    print(\"%r %r \" % (\"Page: \", n_pages))\n",
    "\n",
    "    #4. Web Request and Response\n",
    "    #Make a simple GET request (just fetching a page)\n",
    "    try:\n",
    "        r = requests.get(site_url, headers=headers, timeout=60)  # wait up to 60 seconds\n",
    "    # explicitly handle timeout\n",
    "    except requests.exceptions.Timeout:\n",
    "        print('Connection timed out after 60 seconds, please try later')\n",
    "        continue #skip onto next iteration of for loop\n",
    "    # explicitly handle other network issues\n",
    "    except requests.exceptions.RequestException:\n",
    "        print('Network error occured, please try later')\n",
    "        continue  # skip onto next iteration of for loop\n",
    "\n",
    "    #Check response code from the web server, a code of 200 is good, 4XX or 5XX errors are indicative of something gone wrong\n",
    "    print(\"%r %r \" % (\"HTTP Status Code: \", r.status_code))\n",
    "\n",
    "    #use r.text to access the returned HTML of the page in a single string\n",
    "    #convert the HTML text from the response into a BeautifulSoup Document Object Model'ish structure that can be queried\n",
    "    page_html = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    #5. Parse Data\n",
    "    course_section = page_html.find_all(class_=\"course accordian\")\n",
    "    if course_section != []:\n",
    "        course_details = course_section[0].find_all('article')\n",
    "        for course_id, course in enumerate(course_details):\n",
    "            accordion_id = \"accordion_\" + str(course_id + 1) #construct ID of the HTML elements in course accordion div\n",
    "            further_info_dict = {}\n",
    "            try:\n",
    "                course_title = course.find('label', {'for': accordion_id}).text.replace('\\n', '')\n",
    "                # e.g. Business Risk Management Short Course\n",
    "            except IndexError:\n",
    "                course_title = 'null'\n",
    "\n",
    "            try:\n",
    "                course_description = course.find('div', {'class': 'content'}).text.replace('\\n', '')\n",
    "                # e.g. This 10 week course...\n",
    "            except IndexError:\n",
    "                course_description = 'null'\n",
    "\n",
    "            course_further_details = course.find_all('li')\n",
    "            for i, course_further_item in enumerate(course_further_details):\n",
    "                try:\n",
    "                    course_further_item_description = course_further_item.find_all('a', href=True)[0].contents[0]\n",
    "                except IndexError:\n",
    "                    course_further_item_description = \"\"\n",
    "\n",
    "                try:\n",
    "                    course_further_item_url = course_further_item.find_all('a', href=True)[0]['href']\n",
    "                except IndexError:\n",
    "                    course_further_item_url = \"\"\n",
    "\n",
    "                further_info_dict[i] = [course_further_item_description, course_further_item_url]\n",
    "\n",
    "            #update lists with column data, these lists need to be same size\n",
    "            title.append(course_title)\n",
    "            description.append(course_description)\n",
    "            source_url.append(site_url)\n",
    "\n",
    "            #we assume maximum of 4 further info items for each course\n",
    "            further_info1 = further_info_dict.get(0, empty_list)\n",
    "            further_info2 = further_info_dict.get(1, empty_list)\n",
    "            further_info3 = further_info_dict.get(2, empty_list)\n",
    "            further_info4 = further_info_dict.get(3, empty_list)\n",
    "\n",
    "            further_info1_description.append(further_info1[0])\n",
    "            further_info1_url.append(further_info1[1])\n",
    "            further_info2_description.append(further_info2[0])\n",
    "            further_info2_url.append(further_info2[1])\n",
    "            further_info3_description.append(further_info3[0])\n",
    "            further_info3_url.append(further_info3[1])\n",
    "            further_info4_description.append(further_info4[0])\n",
    "            further_info4_url.append(further_info4[1])\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Sleep for 1 or 2 seconds before requesting the next url\n",
    "    time.sleep(randint(1, 2))\n",
    "\n",
    "print('Summary: scraped {} pages containing {} courses.'.format(n_pages, len(title)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Persist Data\n",
    "\n",
    "Once the major work is complete i.e. the Web Scraping, we now have all the data held in memory (more specifically in the different column lists we created earlier). The object of Web Scraping exercises is often to retrieve data which will the be analysed and therefore it makes sense to persist the data. One could persist the data in a database table or in a flat file (Excel or csv). \n",
    "\n",
    "In this example, we proceed to persist the data in a csv file. To do this, we make use of a nifty library called Pandas. Pandas makes it easy to work with data in tabular form, more specificalyly in a Data Frame. We combine the various column lists into a single Data Frame. Once we have a Pandas Data Frame, it is very easy to save the data as a csv file by calling the `to_csv` method on the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_data_raw_20190510-144324.csv successfully saved to /Users/juliankanjere/Documents/Academic/MPhil Data Science/Course Material/DOC5039F Financial Software Engineering/KNJJUL001\n"
     ]
    }
   ],
   "source": [
    "#  save all these variables in a single dataframe\n",
    "column_names = ['Title', 'Description', 'Source URL', 'Further Info 1', 'Further Info 1 URL', 'Further Info 2',\n",
    "        'Further Info 2 URL','Further Info 3', 'Further Info 3 URL', 'Further Info 4', 'Further Info 4 URL']\n",
    "\n",
    "course_data = pd.DataFrame({\n",
    "                            'Title': title,\n",
    "                            'Description': description,\n",
    "                            'Source URL': source_url,\n",
    "                            'Further Info 1': further_info1_description,\n",
    "                            'Further Info 1 URL': further_info1_url,\n",
    "                            'Further Info 2': further_info2_description,\n",
    "                            'Further Info 2 URL': further_info2_url,\n",
    "                            'Further Info 3': further_info3_description,\n",
    "                            'Further Info 3 URL': further_info3_url,\n",
    "                            'Further Info 4': further_info4_description,\n",
    "                            'Further Info 4 URL': further_info4_url\n",
    "                            })[column_names]\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "filename = 'course_data_raw_' + timestr + '.csv'\n",
    "currentDirectory = os.getcwd()\n",
    "try:\n",
    "    course_data.to_csv(filename, index=None, header=True)\n",
    "    print('{} successfully saved to {}'.format(filename, currentDirectory ))\n",
    "except:\n",
    "    print('Error occured, {} was not saved to {}. Please try again.'.format(filename,currentDirectory ))\n",
    "#to read the csv file\n",
    "#course_data.read_csv(filename)\n",
    "\n",
    "#alternatively you can write to an Excel file \n",
    "#you will need xlwt library to generate spreadsheet files compatible with Microsoft Excel versions 95 to 2003.\n",
    "#filename = 'course_data_raw_' + timestr + '.xls'\n",
    "#course_data.to_excel(filename)\n",
    "#to read the Excel file\n",
    "#course_data.read_excel(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other Considerations\n",
    "Other considerations that have not been discussed in this tutorial with respect to Web Scraping include permissions, use of sessions and user of proxy servers. These are briefly discussed below.  \n",
    "\n",
    "**Permissions**\n",
    "\n",
    "In some cases, one might require permission from a Website's Administrator to scrap their site for data. It is ecommended to request approval and ensuring that one is not in contravention of any cyber laws specific to the jurisdiction they find themselves in before beginning to scrap a website.  \n",
    "\n",
    "**Sessions**\n",
    "\n",
    "HTTP is generally stateless(i.e. Web requests are independent of each other). However, some websites require a user to login in order to access a protected section of the website. These websites then employ some form of a mechanism to identify the logged in user across multiple requests. This ican be achived by use of cookies - when a user first logs in successfully o a website, a session cookie is set that is then used to identify the user to the website. As long as future requests send this cookie along, the site knows the user and what functionality they have got access to. In some cases, when performing Web Scraping, a uer might require to be logged in - in order to scrap information that they would otherwise not have access to if they were not logged in. The code below shows an example of sessions in Python, using the requests library.\n",
    "\n",
    "\n",
    "**Proxy Server**\n",
    "\n",
    "The Web Server that one access whilst scraping will be able to see the source IP address of the Web Request. The source IP address can be masked by use of a Proxy Server. The python requests library also supports proxy servers. See an example below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############Sessions#########################\n",
    "# create a session\n",
    "# session = requests.Session()\n",
    "\n",
    "# use session to make a login POST request (i.e. sending email and password hence POST)\n",
    "# session.post(\"http://apple.com/login\", data=dict(email=\"steve@apple.com\",password=\"jobs\"))\n",
    "\n",
    "# assumming successful login, subsequent requests using the session will automatically include the cookie\n",
    "#r = session.get(\"http://apple.com/user_only_content\")\n",
    "\n",
    "\n",
    "############Proxy Servers#####################\n",
    "# assumming proxy server is setup on localhost (127.0.0.1) \n",
    "#r = requests.get(\"http://apple.com/user_only_content\", proxies=dict(http=\"http://proxy_username:proxy_password@127.0.0.1:port\",\n",
    "#))\n",
    "\n",
    "##############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "In this tutorial, we have explored Web Scraping using Python. The main libraries used in this exercise were `Beautiful Soup`, `requests` and `Pandas`. We scraped Course Information from the AIFMRM online short courses web page and persisted this data in csv format on the file disk.\n",
    "\n",
    "A great next step would be building on this to create a Web Scraper to pull cars for sale from a listings website and downloadig the images of the cars. After all, the best way to learn anything is by doing.\n",
    "\n",
    "Good Luck and Happy Scraping!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}